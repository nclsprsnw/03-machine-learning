{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164ac103",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48741ea0",
   "metadata": {},
   "source": [
    "## Logistic Regression (with one column in the dataset)\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.6993031981418617\n",
    "* f1-score on test set:  0.6852048397367863\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9831027489407843\n",
    "* accuracy on test set :  0.9826293719399348\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.6091828793774319\n",
    "* recall-score on training set:  0.5862695241554667\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8207171314741036\n",
    "* precision-score on test set:  0.8243105209397344\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Moderate Overall Performance for the \"Positive\" Class: The f1-scores of approximately 0.69 on the training set and 0.68 on the test set indicate a moderate level of effectiveness in classifying the positive class. While not poor, these scores suggest there's significant room for improvement in correctly identifying and predicting the \"positive\" instances.\n",
    "\n",
    "Consistency and Generalization (Positive): The f1-score on the test set (0.685) is very close to the training set (0.699). This minimal drop confirms that the model is generalizing well and is not overfitting to the training data. The model's ability to perform consistently on unseen data is a strength, implying that the observed f1-score is a reliable indicator of its real-world performance for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a5f79",
   "metadata": {},
   "source": [
    "## Logistic Regression (with the full dataset)\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.764031382015691\n",
    "* f1-score on test set:  0.7585788035226237\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862603333011218\n",
    "* accuracy on test set :  0.9860320472274932\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.6897303187142468\n",
    "* recall-score on training set:  0.6802832244008714\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8562732499154548\n",
    "* precision-score on test set:  0.8572409059711736\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Improved Overall Performance for the \"Positive\" Class: Compared to the single-column model (f1-score ~0.69), training with the full dataset has led to a noticeable improvement in the f1-score, rising to approximately 0.76 on both training and test sets. This indicates that the model is now significantly more effective at correctly identifying and predicting the \"positive\" instances, striking a better balance between precision and recall.\n",
    "\n",
    "Strong Consistency and Generalization: The f1-score on the test set (0.759) remains very close to the training set (0.764). This minimal difference demonstrates excellent generalization, confirming that the model is not overfitting to the training data and maintains its improved performance on unseen data. This consistency makes the observed f1-score a reliable indicator of the model's true performance for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d4bc3",
   "metadata": {},
   "source": [
    "## Logistic Regression (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7642619981889526\n",
    "* f1-score on test set:  0.7583485124468731\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862779032073864\n",
    "* accuracy on test set :  0.986014477475578\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.6897303187142468\n",
    "* recall-score on training set:  0.6802832244008714\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8568527918781725\n",
    "* precision-score on test set:  0.8566529492455418\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'C': 0.8442234478755427, 'penalty': 'l1', 'max_iter': 274}\n",
    "```\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Consistent and Strong Generalization: The f1-score on the test set (0.758) is very close to the training set (0.764). This minimal difference indicates excellent generalization and confirms that the model is not overfitting to the training data. The optimized Logistic Regression model is highly consistent in its performance on unseen data, making its f1-score a reliable indicator of its real-world effectiveness for the positive class.\n",
    "\n",
    "Solid Overall Performance for the \"Positive\" Class: An f1-score of approximately 0.758 on the test set signifies a strong and balanced performance in classifying the positive class. It suggests the model is making a good trade-off between accurately identifying true positives (recall) and ensuring that its positive predictions are correct (precision).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876926d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Random Forest (with the full dataset) - no hyperparameters optimisation\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.8062858419602293\n",
    "* f1-score on test set:  0.7351802204349122\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9885751684514763\n",
    "* accuracy on test set :  0.9843804905474735\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.7372650503949877\n",
    "* recall-score on training set:  0.6721132897603486\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8895645028759244\n",
    "* precision-score on test set:  0.8113083497698882\n",
    "\n",
    "### Understandings\n",
    "\n",
    "High Training Performance, Moderate Test Performance: The f1-score on the training set (0.806) is very high, indicating that the Random Forest model, by default, is excellent at classifying the positive class on the data it has seen. However, the f1-score on the test set drops noticeably to 0.735. This disparity is a key observation.\n",
    "\n",
    "Evidence of Overfitting (Primary Concern): The significant drop in f1-score from training to test set (0.806 to 0.735) is a strong indicator of overfitting. The model has learned the training data too well, including its noise and specific patterns, and is therefore not generalizing as effectively to unseen data. This means its impressive performance on the training set is not fully translating to real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c3be8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Random Forest (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7768993298697388\n",
    "* f1-score on test set:  0.7546142208774584\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9869850919345345\n",
    "* accuracy on test set :  0.9857509311968515\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.7026695723236176\n",
    "* recall-score on training set:  0.6791938997821351\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8686647583768311\n",
    "* precision-score on test set:  0.8488767869298843\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 345, 'max_features': 'log2', 'class_weight': None, 'criterion': 'gini'}\n",
    "```\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Improved Generalization and Reduced Overfitting (Key Success): The most significant observation is the much closer f1-score between the training set (0.777) and the test set (0.755) compared to the Random Forest model without hyperparameter optimization (training: 0.806, test: 0.735). This much smaller gap indicates that hyperparameter optimization has been highly effective in mitigating overfitting. The model is now generalizing much better to unseen data.\n",
    "\n",
    "Solid Overall Performance for the \"Positive\" Class: An f1-score of approximately 0.755 on the test set signifies a strong and balanced performance in classifying the positive class. This score suggests the model is making a good trade-off between identifying true positives (recall) and ensuring its positive predictions are accurate (precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92be82a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# XGBoost (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "## Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7656261696234749\n",
    "* f1-score on test set:  0.7584541062801933\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862471558714234\n",
    "* accuracy on test set :  0.9859441984679176\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.696540452192863\n",
    "* recall-score on training set:  0.6840958605664488\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8499252118996178\n",
    "* precision-score on test set:  0.8509485094850948\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'lambda': 5.447463756314041e-06, 'alpha': 0.0048705270812827725, 'colsample_bytree': 0.786726456107845, 'subsample': 0.5131849780000297, 'learning_rate': 0.015917236602439464, 'n_estimators': 765, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.00033246769032163886}\n",
    "```\n",
    "\n",
    "## Understandings\n",
    "\n",
    "Excellent Generalization with Minimal Overfitting: The f1-score on the test set (0.758) is very close to the training set (0.766). This minimal difference demonstrates excellent generalization, indicating that the hyperparameter optimization has been successful in preventing overfitting. The model performs consistently on unseen data, making its f1-score a reliable indicator of its real-world effectiveness for the positive class.\n",
    "\n",
    "Strong Overall Performance for the \"Positive\" Class: An f1-score of approximately 0.758 on the test set signifies a strong and balanced performance in classifying the positive class. This score suggests the model is making a good trade-off between accurately identifying true positives (recall) and ensuring its positive predictions are correct (precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19da1c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64001b5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Logistic Regression (with the full dataset)\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7641786548166002\n",
    "* f1-score on test set:  0.7603155339805825\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862471558714234\n",
    "* accuracy on test set :  0.9861198959870686\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.6909561427403977\n",
    "* recall-score on training set:  0.6824618736383442\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.854759898904802\n",
    "* precision-score on test set:  0.8582191780821918\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Strong and Consistent Generalization: The f1-score on the test set (0.760) is extremely close to the training set (0.764). This minimal difference signifies excellent generalization and confirms that the model is not overfitting to the training data. The model performs consistently on unseen data, making its f1-score a reliable indicator of its real-world effectiveness for the positive class.\n",
    "\n",
    "Slight Improvement in Test F1-score: Comparing to the previous Logistic Regression model (full dataset, optimized, test f1: 0.758), this model with engineered features shows a marginal, but positive, increase in test f1-score (0.760). While small, this suggests that the newly added features (total_pages_visited_x2, total_pages_visited_age, total_pages_visited_age_x2) have contributed ever so slightly to the model's ability to better classify the positive class on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dab2a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Logistic Regression (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.764319673986869\n",
    "* f1-score on test set:  0.7588717015468608\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862822956839525\n",
    "* accuracy on test set :  0.9860320472274932\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.6897303187142468\n",
    "* recall-score on training set:  0.6813725490196079\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8569977999661533\n",
    "* precision-score on test set:  0.8562628336755647\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'C': 1.993327899763547, 'penalty': 'l1', 'max_iter': 812}\n",
    "```\n",
    "\n",
    "### Understandings\n",
    "\n",
    "Strong Generalization and Minimal Overfitting: The f1-score on the test set (0.759) is exceptionally close to the training set (0.764). This minimal difference strongly indicates excellent generalization and confirms that the model is not overfitting to the training data. The optimized Logistic Regression model, even with new features, performs consistently on unseen data, making its f1-score a reliable indicator of its real-world effectiveness for the positive class.\n",
    "\n",
    "Consistent and High Performance for the \"Positive\" Class: An f1-score of approximately 0.759 on the test set signifies a strong and balanced performance in classifying the positive class. This score suggests the model is making a good trade-off between accurately identifying true positives (recall) and ensuring its positive predictions are correct (precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750c472",
   "metadata": {},
   "source": [
    "## Random Forest (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "### Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7768993298697388\n",
    "* f1-score on test set:  0.7546142208774584\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9869850919345345\n",
    "* accuracy on test set :  0.9857509311968515\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.7026695723236176\n",
    "* recall-score on training set:  0.6791938997821351\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8686647583768311\n",
    "* precision-score on test set:  0.8488767869298843\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 345, 'max_features': 'log2', 'class_weight': None, 'criterion': 'gini'}\n",
    "```\n",
    "\n",
    "### Understandings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c329d",
   "metadata": {},
   "source": [
    "# XGBoost (with the full dataset) - with hyperparameters optimisation\n",
    "\n",
    "## Results\n",
    "\n",
    "f1-score:\n",
    "\n",
    "* f1-score on training set:  0.7656261696234749\n",
    "* f1-score on test set:  0.7584541062801933\n",
    "\n",
    "accuracy:\n",
    "\n",
    "* accuracy on training set :  0.9862471558714234\n",
    "* accuracy on test set :  0.9859441984679176\n",
    "\n",
    "recall:\n",
    "\n",
    "* recall-score on training set:  0.696540452192863\n",
    "* recall-score on training set:  0.6840958605664488\n",
    "\n",
    "precision:\n",
    "\n",
    "* precision-score on training set:  0.8499252118996178\n",
    "* precision-score on test set:  0.8509485094850948\n",
    "\n",
    "hyperparameters:\n",
    "\n",
    "```json\n",
    "{'lambda': 5.447463756314041e-06, 'alpha': 0.0048705270812827725, 'colsample_bytree': 0.786726456107845, 'subsample': 0.5131849780000297, 'learning_rate': 0.015917236602439464, 'n_estimators': 765, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.00033246769032163886}\n",
    "```\n",
    "\n",
    "## Understandings\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
